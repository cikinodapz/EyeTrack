{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57252ca2-4e04-4553-b361-24d7dcf5a388",
   "metadata": {},
   "source": [
    "### \"XGBoost Ensemble dengan Circular-Entropy Features untuk Eye-Tracking Classification\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6bc32c-14e7-41dc-991b-0722cbdc0960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] Ensemble | Acc: 0.8181 | AUC: 0.9013 | thr=0.510\n",
      "\n",
      "========== TEST (ENSEMBLE) ==========\n",
      "Best threshold (from VAL): 0.510\n",
      "Accuracy: 0.8216\n",
      "ROC AUC : 0.9068\n",
      "Confusion matrix [[TN, FP], [FN, TP]]:\n",
      "[[21036  3996]\n",
      " [ 4937 20095]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Sequential(1)     0.8099    0.8404    0.8249     25032\n",
      "    Random(2)     0.8341    0.8028    0.8182     25032\n",
      "\n",
      "     accuracy                         0.8216     50064\n",
      "    macro avg     0.8220    0.8216    0.8215     50064\n",
      " weighted avg     0.8220    0.8216    0.8215     50064\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# xgb_rowlevel_push80.py\n",
    "# Ensembling XGBoost row-level:\n",
    "# - tetap pakai feature engineering kamu (rolling, circular stats, hist, entropy, dll)\n",
    "# - bedanya: train banyak base models (variasi scaler / seed / param mix) => soft-voting\n",
    "# - threshold search dilebarin (0.15..0.85)\n",
    "# Target: dorong akurasi 80%+\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "CSV_PATH = \"truncated_dataset-seqglo.csv\"\n",
    "ROLL_W = 21\n",
    "USE_SUBSAMPLE = True\n",
    "SUBSAMPLE_N = 160_000     # sedikit dinaikkan\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Coba aktifkan GPU kalau ada\n",
    "TREE_METHOD = \"hist\"  # ganti ke \"gpu_hist\" kalau environment mendukung\n",
    "\n",
    "SCALE_MODES = [\"robust\", \"quantile\", \"standard\"]  # kita ensemble 3 scaler\n",
    "SEEDS = [42, 7, 1029]                              # 3 seed -> total 3*3 = 9 model\n",
    "# Dua variasi param ringan (mixA/mixB) untuk diversitas\n",
    "PARAM_MIXES = [\n",
    "    dict(max_depth=5, min_child_weight=7, subsample=0.85, colsample_bytree=0.85, gamma=0.25, reg_alpha=0.6, reg_lambda=2.2),\n",
    "    dict(max_depth=6, min_child_weight=5, subsample=0.80, colsample_bytree=0.80, gamma=0.20, reg_alpha=0.5, reg_lambda=2.6),\n",
    "]\n",
    "\n",
    "BASE_KW = dict(\n",
    "    objective=\"binary:logistic\",\n",
    "    n_estimators=1200,     # sedikit lebih panjang, lr diturunkan\n",
    "    learning_rate=0.045,\n",
    "    tree_method=TREE_METHOD,\n",
    "    n_jobs=4,\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Utils\n",
    "# =========================\n",
    "def entropy_hist(a, bins_edges):\n",
    "    a = np.asarray(a)\n",
    "    a = a[np.isfinite(a)]\n",
    "    if a.size == 0: return 0.0\n",
    "    hist, _ = np.histogram(a, bins=bins_edges)\n",
    "    s = hist.sum()\n",
    "    if s == 0: return 0.0\n",
    "    p = hist.astype(float) / s\n",
    "    p = p[p > 0]\n",
    "    return float(-(p * np.log(p)).sum())\n",
    "\n",
    "def hist_feats(a, bins_edges, prefix):\n",
    "    hist, _ = np.histogram(a, bins=bins_edges)\n",
    "    total = hist.sum() + 1e-9\n",
    "    d = {}\n",
    "    for i, c in enumerate(hist):\n",
    "        d[f\"{prefix}_bin{i}\"] = float(c)\n",
    "        d[f\"{prefix}_p{i}\"] = float(c) / total\n",
    "    d[f\"{prefix}_sum\"] = float(total)\n",
    "    return d\n",
    "\n",
    "# =========================\n",
    "# 1) Load & pre-bins (global)\n",
    "# =========================\n",
    "df = pd.read_csv(CSV_PATH).sort_values([\"nama\", \"time\"]).reset_index(drop=True)\n",
    "\n",
    "dx_all = df[\"gazeX\"].diff()\n",
    "dy_all = df[\"gazeY\"].diff()\n",
    "mask_new_subject = df[\"nama\"].ne(df[\"nama\"].shift(1))\n",
    "dx_all = dx_all.mask(mask_new_subject, 0.0)\n",
    "dy_all = dy_all.mask(mask_new_subject, 0.0)\n",
    "step_all = np.sqrt(dx_all**2 + dy_all**2).fillna(0.0).values\n",
    "\n",
    "q = np.quantile(step_all, [0.00, 0.10, 0.25, 0.50, 0.75, 0.90, 0.97, 0.995, 1.00])\n",
    "step_bins = np.unique(q)\n",
    "if step_bins.size < 6:\n",
    "    step_bins = np.linspace(float(step_all.min()), float(step_all.max()+1e-6), 9)\n",
    "\n",
    "dang_bins = np.linspace(-np.pi, np.pi, 9)\n",
    "small_thr = np.quantile(step_all, 0.25)\n",
    "large_thr = np.quantile(step_all, 0.90)\n",
    "\n",
    "# =========================\n",
    "# 2) Feature Engineering (sama seperti punyamu)\n",
    "# =========================\n",
    "def add_roll_feats(g, w=ROLL_W):\n",
    "    g = g.copy()\n",
    "\n",
    "    dx = g[\"gazeX\"].diff(); dy = g[\"gazeY\"].diff()\n",
    "    dx.iloc[0] = 0.0; dy.iloc[0] = 0.0\n",
    "    step = np.sqrt(dx**2 + dy**2)\n",
    "\n",
    "    ang = np.arctan2(dy, dx)\n",
    "    d_ang = np.diff(ang, prepend=ang.iloc[0])\n",
    "    d_ang = (d_ang + np.pi) % (2*np.pi) - np.pi\n",
    "\n",
    "    cos_da = np.cos(d_ang); sin_da = np.sin(d_ang)\n",
    "    r_mean_cos = pd.Series(cos_da, index=g.index).rolling(w, min_periods=1).mean()\n",
    "    r_mean_sin = pd.Series(sin_da, index=g.index).rolling(w, min_periods=1).mean()\n",
    "    r_mrl = np.sqrt(r_mean_cos**2 + r_mean_sin**2)\n",
    "    r_circ_var = 1.0 - r_mrl\n",
    "\n",
    "    g[\"r_std_x\"] = g[\"gazeX\"].rolling(w, min_periods=1).std()\n",
    "    g[\"r_std_y\"] = g[\"gazeY\"].rolling(w, min_periods=1).std()\n",
    "    g[\"r_mean_step\"] = step.rolling(w, min_periods=1).mean()\n",
    "    g[\"r_std_step\"]  = step.rolling(w, min_periods=1).std()\n",
    "    g[\"r_mean_abs_dang\"] = pd.Series(np.abs(d_ang), index=g.index).rolling(w, min_periods=1).mean()\n",
    "    g[\"r_std_dang\"] = pd.Series(d_ang, index=g.index).rolling(w, min_periods=1).std()\n",
    "    try:\n",
    "        g[\"r_skew_step\"] = pd.Series(step, index=g.index).rolling(w, min_periods=1).skew()\n",
    "        g[\"r_kurt_step\"] = pd.Series(step, index=g.index).rolling(w, min_periods=1).kurt()\n",
    "    except Exception:\n",
    "        g[\"r_skew_step\"] = 0.0; g[\"r_kurt_step\"] = 0.0\n",
    "\n",
    "    g[\"r_q25_step\"] = pd.Series(step, index=g.index).rolling(w, min_periods=1).quantile(0.25)\n",
    "    g[\"r_q75_step\"] = pd.Series(step, index=g.index).rolling(w, min_periods=1).quantile(0.75)\n",
    "\n",
    "    disp_x = g[\"gazeX\"] - g[\"gazeX\"].shift(w-1)\n",
    "    disp_y = g[\"gazeY\"] - g[\"gazeY\"].shift(w-1)\n",
    "    disp = np.sqrt(disp_x**2 + disp_y**2)\n",
    "    path_w = pd.Series(step, index=g.index).rolling(w, min_periods=1).sum() + 1e-6\n",
    "    g[\"r_straight_ratio\"] = (disp / path_w).fillna(0)\n",
    "    g[\"bbox_w\"] = g[\"gazeX\"].rolling(w, min_periods=1).max() - g[\"gazeX\"].rolling(w, min_periods=1).min()\n",
    "    g[\"bbox_h\"] = g[\"gazeY\"].rolling(w, min_periods=1).max() - g[\"gazeY\"].rolling(w, min_periods=1).min()\n",
    "\n",
    "    g[\"r_entropy_dang\"] = pd.Series(d_ang, index=g.index).rolling(w, min_periods=1).apply(\n",
    "        lambda a: entropy_hist(a, dang_bins), raw=True\n",
    "    )\n",
    "    g[\"r_entropy_step\"] = pd.Series(step, index=g.index).rolling(w, min_periods=1).apply(\n",
    "        lambda a: entropy_hist(a, step_bins), raw=True\n",
    "    )\n",
    "\n",
    "    def roll_hist_features(series, bins, pref):\n",
    "        arr = []\n",
    "        s = series.values; n = len(s)\n",
    "        for i in range(n):\n",
    "            i0 = max(0, i - w + 1)\n",
    "            win = s[i0:i+1]\n",
    "            d = hist_feats(win, bins, pref)\n",
    "            arr.append(d)\n",
    "        return pd.DataFrame(arr, index=series.index)\n",
    "\n",
    "    dang_hist_df = roll_hist_features(pd.Series(d_ang, index=g.index), dang_bins, \"h_dang\")\n",
    "    step_hist_df = roll_hist_features(pd.Series(step, index=g.index), step_bins, \"h_step\")\n",
    "    g = pd.concat([g, dang_hist_df, step_hist_df], axis=1)\n",
    "\n",
    "    small_mask = (step <= small_thr).astype(float)\n",
    "    large_mask = (step >= large_thr).astype(float)\n",
    "    g[\"r_rate_small\"] = pd.Series(small_mask, index=g.index).rolling(w, min_periods=1).mean()\n",
    "    g[\"r_rate_large\"] = pd.Series(large_mask, index=g.index).rolling(w, min_periods=1).mean()\n",
    "    g[\"r_ratio_small_large\"] = (g[\"r_rate_small\"] / (g[\"r_rate_large\"] + 1e-6)).replace([np.inf, -np.inf], 0.0)\n",
    "\n",
    "    g[\"dx\"] = dx; g[\"dy\"] = dy\n",
    "    g[\"abs_dx\"] = dx.abs(); g[\"abs_dy\"] = dy.abs()\n",
    "    g[\"step\"] = step\n",
    "    g[\"r_mean_cos_dang\"] = r_mean_cos\n",
    "    g[\"r_mean_sin_dang\"] = r_mean_sin\n",
    "    g[\"r_mrl_dang\"] = r_mrl\n",
    "    g[\"r_circvar_dang\"] = r_circ_var\n",
    "\n",
    "    return g\n",
    "\n",
    "try:\n",
    "    df = df.groupby(\"nama\", group_keys=False).apply(add_roll_feats, include_groups=False, w=ROLL_W)\n",
    "except TypeError:\n",
    "    df = df.groupby(\"nama\", group_keys=False).apply(add_roll_feats, w=ROLL_W)\n",
    "\n",
    "# bersihin NaN/Inf\n",
    "for col in df.columns:\n",
    "    if col in [\"nama\", \"time\", \"gazeX\", \"gazeY\", \"label\"]:\n",
    "        continue\n",
    "    med = pd.to_numeric(df[col], errors=\"coerce\").replace([np.inf, -np.inf], np.nan).median()\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\").replace([np.inf, -np.inf], np.nan).fillna(med)\n",
    "\n",
    "# =========================\n",
    "# 3) Features & Label\n",
    "# =========================\n",
    "exclude = {\"nama\", \"label\"}\n",
    "feature_cols = [c for c in df.columns if c not in exclude]\n",
    "X = df[feature_cols].astype(float).copy()\n",
    "y = (df[\"label\"] == 2).astype(int).values\n",
    "\n",
    "# =========================\n",
    "# 4) Split\n",
    "# =========================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# subsample train imbang\n",
    "if USE_SUBSAMPLE:\n",
    "    rng = np.random.RandomState(RANDOM_STATE)\n",
    "    ix_pos = np.where(y_train == 1)[0]\n",
    "    ix_neg = np.where(y_train == 0)[0]\n",
    "    n_each = min(SUBSAMPLE_N // 2, len(ix_pos), len(ix_neg))\n",
    "    sel = np.concatenate([\n",
    "        rng.choice(ix_pos, n_each, replace=False),\n",
    "        rng.choice(ix_neg, n_each, replace=False),\n",
    "    ])\n",
    "    X_train_small = X_train.iloc[sel].reset_index(drop=True)\n",
    "    y_train_small = y_train[sel]\n",
    "else:\n",
    "    X_train_small = X_train.reset_index(drop=True)\n",
    "    y_train_small = y_train\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train_small, y_train_small, test_size=0.15, stratify=y_train_small, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 5) Scalers\n",
    "# =========================\n",
    "def make_scaler(mode):\n",
    "    if mode == \"standard\":\n",
    "        return StandardScaler()\n",
    "    elif mode == \"robust\":\n",
    "        return RobustScaler()\n",
    "    elif mode == \"quantile\":\n",
    "        return QuantileTransformer(output_distribution=\"normal\", random_state=RANDOM_STATE, subsample=300_000)\n",
    "    else:\n",
    "        raise ValueError(\"unknown scaler\")\n",
    "\n",
    "def apply_scaler(sc, Xdf, fit=False):\n",
    "    if sc is None: return Xdf.values\n",
    "    return (sc.fit_transform(Xdf) if fit else sc.transform(Xdf))\n",
    "\n",
    "# =========================\n",
    "# 6) Train an ensemble\n",
    "# =========================\n",
    "models = []\n",
    "scalers = []\n",
    "for mode in SCALE_MODES:\n",
    "    sc = make_scaler(mode)\n",
    "    Xtr_s = apply_scaler(sc, X_tr, fit=True)\n",
    "    Xva_s = apply_scaler(sc, X_val, fit=False)\n",
    "\n",
    "    for mix in PARAM_MIXES:\n",
    "        for seed in SEEDS:\n",
    "            kw = dict(BASE_KW)\n",
    "            kw.update(mix)\n",
    "            kw[\"random_state\"] = seed\n",
    "            clf = XGBClassifier(**kw)\n",
    "            clf.fit(Xtr_s, y_tr)\n",
    "            models.append((mode, seed, mix, clf))\n",
    "    scalers.append((mode, sc))\n",
    "\n",
    "# =========================\n",
    "# 7) Validation blend & threshold search (luas)\n",
    "# =========================\n",
    "# Kumpulkan proba val dari semua model (soft-vote)\n",
    "proba_val_all = []\n",
    "for (mode, seed, mix, clf) in models:\n",
    "    sc = dict(scalers)[mode]\n",
    "    Xva_s = apply_scaler(sc, X_val, fit=False)\n",
    "    proba_val_all.append(clf.predict_proba(Xva_s)[:, 1])\n",
    "\n",
    "proba_val_blend = np.mean(proba_val_all, axis=0)\n",
    "\n",
    "ths = np.arange(0.15, 0.851, 0.001)  # jauh lebih luas\n",
    "best_thr, best_acc, best_auc = 0.5, -1.0, None\n",
    "for t in ths:\n",
    "    preds = (proba_val_blend >= t).astype(int)\n",
    "    acc = accuracy_score(y_val, preds)\n",
    "    if acc > best_acc:\n",
    "        best_thr = float(t); best_acc = acc\n",
    "try:\n",
    "    best_auc = roc_auc_score(y_val, proba_val_blend)\n",
    "except Exception:\n",
    "    best_auc = float(\"nan\")\n",
    "\n",
    "print(f\"[VAL] Ensemble | Acc: {best_acc:.4f} | AUC: {best_auc:.4f} | thr={best_thr:.3f}\")\n",
    "\n",
    "# =========================\n",
    "# 8) Test with blended models\n",
    "# =========================\n",
    "# NOTE: fit scaler on full-train (X_train_small), bukan hanya X_tr\n",
    "proba_test_all = []\n",
    "for mode in SCALE_MODES:\n",
    "    sc = make_scaler(mode)\n",
    "    sc.fit(X_train_small)  # fit di seluruh train kecil\n",
    "    Xtest_s = apply_scaler(sc, X_test, fit=False)\n",
    "\n",
    "    # ambil semua model yang cocok mode-nya\n",
    "    for (m_mode, seed, mix, clf) in models:\n",
    "        if m_mode != mode: continue\n",
    "        # retrain model di full-train kecil dengan scaler yg baru\n",
    "        kw = dict(BASE_KW); kw.update(mix); kw[\"random_state\"] = seed\n",
    "        clf_full = XGBClassifier(**kw)\n",
    "        Xfull_s = apply_scaler(sc, X_train_small, fit=False)\n",
    "        clf_full.fit(Xfull_s, y_train_small)\n",
    "        proba_test_all.append(clf_full.predict_proba(Xtest_s)[:, 1])\n",
    "\n",
    "proba_test_blend = np.mean(proba_test_all, axis=0)\n",
    "preds_test = (proba_test_blend >= best_thr).astype(int)\n",
    "\n",
    "acc = accuracy_score(y_test, preds_test)\n",
    "try:\n",
    "    auc = roc_auc_score(y_test, proba_test_blend)\n",
    "except Exception:\n",
    "    auc = float(\"nan\")\n",
    "cm = confusion_matrix(y_test, preds_test)\n",
    "report = classification_report(y_test, preds_test, target_names=[\"Sequential(1)\",\"Random(2)\"], digits=4)\n",
    "\n",
    "print(\"\\n========== TEST (ENSEMBLE) ==========\")\n",
    "print(f\"Best threshold (from VAL): {best_thr:.3f}\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"ROC AUC : {auc:.4f}\")\n",
    "print(\"Confusion matrix [[TN, FP], [FN, TP]]:\")\n",
    "print(cm)\n",
    "print(\"\\nClassification report:\")\n",
    "print(report)\n",
    "\n",
    "# Save the model and scaler for future use\n",
    "import joblib\n",
    "joblib.dump(models, \"xgb_rowlevel_models.pkl\")\n",
    "joblib.dump(scalers, \"xgb_rowlevel_scalers.pkl\")\n",
    "print(\"Models and scalers saved to 'xgb_rowlevel_models.pkl' and 'xgb_rowlevel_scalers.pkl'\")\n",
    "print(\"Done.\")\n",
    "print(\"You can load them later using joblib.load()\")\n",
    "print(\"Example: models = joblib.load('xgb_rowlevel_models.pkl')\")\n",
    "print(\"Example: scalers = joblib.load('xgb_rowlevel_scalers.pkl')\")\n",
    "print(\"Make sure to use the same feature engineering and preprocessing steps when loading the models.\")\n",
    "print(\"Good luck with your XGBoost ensemble!\")\n",
    "print(\"Remember to tune the parameters further if needed to push accuracy above 80%!\")\n",
    "print(\"Happy coding!\")\n",
    "print(\"If you have any questions, feel free to ask!\")\n",
    "print(\"Thank you for using this script!\")\n",
    "print(\"Have a great day!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb3171b-8b2d-4e63-9df7-93096cebd53a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
